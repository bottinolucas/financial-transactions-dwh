{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc29cd1c",
   "metadata": {},
   "source": [
    "### Pipeline Raw to Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0aefb8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import pandera.pandas as pa # Validação de dados de DataFrames (semelhante ao Pydantic)\n",
    "from pathlib import Path\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.engine import Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f935cb",
   "metadata": {},
   "source": [
    "#### Definições e Configurações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95f2b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de caminhos \n",
    "DATABASE_URL: str = \"postgresql+psycopg2://admin:admin@localhost:5432/transactions\"\n",
    "DATA_PATH: Path = Path(\"../Data Layer/silver/transactions_cards_users_mcc_fraud.csv\")\n",
    "DDL_PATH: Path = Path(\"scripts/ddl.sql\")\n",
    "\n",
    "# Configuração da engine do SQLAlchemy\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "schema: pa.DataFrameSchema = pa.DataFrameSchema(\n",
    "  columns={   \n",
    "    \"transaction_id\": pa.Column(\n",
    "      pa.Int64,\n",
    "      nullable=False,\n",
    "      checks=pa.Check.greater_than(0)\n",
    "    ),\n",
    "    \"date\": pa.Column(\n",
    "      pa.DateTime, \n",
    "      nullable=False\n",
    "    ),\n",
    "    \"client_id\": pa.Column(\n",
    "      pa.Int64,\n",
    "      nullable=False\n",
    "    ),\n",
    "    \"card_id\": pa.Column(\n",
    "      pa.Int64,\n",
    "      nullable=True\n",
    "    ),\n",
    "    \"amount\": pa.Column(\n",
    "      pa.Float, \n",
    "      nullable=False,\n",
    "      checks=pa.Check.greater_than_or_equal_to(0)\n",
    "    ),\n",
    "    \"mcc\": pa.Column(\n",
    "      pa.Int16,\n",
    "      nullable=False\n",
    "    ),\n",
    "    \"use_chip\": pa.Column(pa.String, nullable=True),\n",
    "    \"merchant_id\": pa.Column(pa.String, nullable=True),\n",
    "    \"merchant_city\": pa.Column(pa.String, nullable=True),\n",
    "    \"merchant_state\": pa.Column(pa.String, nullable=True),\n",
    "    \"zip\": pa.Column(\n",
    "      pa.Int32, \n",
    "      nullable=True, \n",
    "      checks=pa.Check.less_than(99999)\n",
    "    ),\n",
    "    \"errors\": pa.Column(pa.String, nullable=True),\n",
    "    \"card_brand\": pa.Column(pa.String, nullable=True),\n",
    "    \"card_type\": pa.Column(pa.String, nullable=True),\n",
    "    \"card_number\": pa.Column(pa.String, nullable=True),\n",
    "    \"expires\": pa.Column(pa.DateTime, nullable=True),\n",
    "    \"cvv\": pa.Column(\n",
    "      pa.Int32, \n",
    "      nullable=True,\n",
    "      checks=pa.Check.less_than(10000) \n",
    "    ),\n",
    "    \"has_chip\": pa.Column(pa.String, nullable=True),\n",
    "    \"num_cards_issued\": pa.Column(pa.Int16, nullable=True), \n",
    "    \"credit_limit\": pa.Column(pa.Float, nullable=True), \n",
    "    \"acct_open_date\": pa.Column(pa.DateTime, nullable=True),\n",
    "    \"year_pin_last_changed\": pa.Column(pa.String, nullable=True), \n",
    "    \"card_on_dark_web\": pa.Column(pa.String, nullable=True),\n",
    "        \n",
    "    \"current_age\": pa.Column(pa.Int16, nullable=True),\n",
    "    \"retirement_age\": pa.Column(pa.Int16, nullable=True),\n",
    "    \"birth_year\": pa.Column(pa.Int16, nullable=True),\n",
    "    \"birth_month\": pa.Column(pa.Int16, nullable=True), \n",
    "    \"gender\": pa.Column(pa.String, nullable=True),\n",
    "    \"address\": pa.Column(pa.String, nullable=True),\n",
    "    \"latitude\": pa.Column(pa.Float, nullable=True), \n",
    "    \"longitude\": pa.Column(pa.Float, nullable=True), \n",
    "    \"per_capita_income\": pa.Column(pa.Float, nullable=True),\n",
    "    \"yearly_income\": pa.Column(pa.Float, nullable=True), \n",
    "    \"total_debt\": pa.Column(pa.Float, nullable=True), \n",
    "    \"credit_score\": pa.Column(pa.Int16, nullable=True), \n",
    "    \"num_credit_cards\": pa.Column(pa.Int16, nullable=True), \n",
    "\n",
    "    \"mcc_description\": pa.Column(pa.String, nullable=True),\n",
    "    \"is_fraud\": pa.Column(\n",
    "      pa.Int16,\n",
    "      nullable=True,\n",
    "      checks=pa.Check.isin([0, 1]) \n",
    "    )\n",
    "  },\n",
    "  strict=\"filter\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c643ea86",
   "metadata": {},
   "source": [
    "#### Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f94bbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sql_to_string(file: Path) -> str:\n",
    "  \"\"\"\n",
    "  Recebe: Caminho para arquivo SQL (DDL);\n",
    "  Retorna: String com a instrução DDL para criação da tabela.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    with open(file, 'r') as f:\n",
    "      return f.read()\n",
    "  except Exception as e:\n",
    "    print(f\"ERRO! Falha ao encontrar/ler o arquivo: {e}\")\n",
    "    raise\n",
    "\n",
    "def execute_query(sql_query: str, engine: Engine) -> bool:\n",
    "  try:\n",
    "    with engine.connect() as conn:\n",
    "      conn.execute(text(sql_query))\n",
    "      conn.commit()\n",
    "    return True\n",
    "  except Exception as e:\n",
    "    print(f\"ERRO! Falha ao executar a query SQL: {e}\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ad1eed",
   "metadata": {},
   "source": [
    "#### Funções de ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "397ef706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(path: Path) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Recebe: Caminho para arquivo CSV.\n",
    "  Retorna: DataFrame;\n",
    "  \"\"\"\n",
    "  try:\n",
    "    df = pd.read_csv(path)\n",
    "    return df\n",
    "  except Exception as e:\n",
    "    print(f\"ERRO! Um problema ocorreu na conversão do arquivo para DataFrame: {e}\")\n",
    "    return None\n",
    "  \n",
    "def transform_and_validate(df: pd.DataFrame, schema: pa.DataFrameSchema) -> pd.DataFrame:\n",
    "  \"\"\"\n",
    "  Recebe: DataFrame, Schema (Padrão) para validar o DataFrame;\n",
    "  Retorna: DataFrame validado.\n",
    "\n",
    "  O parâmetro lazy=True permite que o pandera execute todas as validações definidas no schema antes de lançar uma possível exceção, coletando todos os erros encontrados.\n",
    "  \"\"\"\n",
    "  try: \n",
    "    df = schema.validate(df, lazy=True)\n",
    "    return df\n",
    "  except Exception as e:\n",
    "    print(f\"ERRO! O DataFrame não foi validado: {e}\")\n",
    "\n",
    "def load(df: pd.DataFrame, ddl_script_path: Path, engine: Engine) -> bool:\n",
    "  \"\"\"\n",
    "  Recebe: DataFrame, Caminho para Script SQL, engine do Postgres\n",
    "  Retorna: True caso sucesso, False caso falhe.\n",
    "  \"\"\"\n",
    "  try:\n",
    "    # 1. Leitura do Script SQL (DDL)\n",
    "    print(f\"Lendo o arquivo DDL: {ddl_script_path}\")\n",
    "    ddl = convert_sql_to_string(ddl_script_path)\n",
    "\n",
    "    # 2. Executando o DDL para criar tabela no PostgreSQL\n",
    "    print(\"Executando DDL...\")\n",
    "    if not execute_query(ddl, engine):\n",
    "      return False\n",
    "\n",
    "    # 4. Populando o Banco de Dados com Pandas\n",
    "    table = ddl_script_path.stem\n",
    "    print(f\"Quantidade de tuplas: {len(df)}, Tabela: {table}\")\n",
    "\n",
    "    df.to_sql(\n",
    "      name=table,\n",
    "      con=engine,\n",
    "      if_exists=\"append\",\n",
    "      index=False,\n",
    "      chunksize=50000,\n",
    "      method=\"multi\"\n",
    "    )\n",
    "\n",
    "    print(\"LOAD no PostgreSQL concluído com sucesso.\")\n",
    "    return True\n",
    "  except Exception as e:\n",
    "    print(f\"ERRO! Processo de LOAD interrompido: {e}\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd019695",
   "metadata": {},
   "source": [
    "#### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "349b9699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(data_path: Path, ddl_script_path: Path, engine: Engine, schema: pa.DataFrameSchema) -> bool:\n",
    "  \"\"\"\n",
    "  Executa o Pipeline \n",
    "  \"\"\"\n",
    "\n",
    "  print(\"ETAPA 01: Extração de dados\")\n",
    "  print(\"Executando...\")\n",
    "  df_raw = extract(data_path)\n",
    "  if df_raw is None: \n",
    "    print(\"FALHA NO PIPELINE: Extração.\")\n",
    "    return False\n",
    "  print(\"Extração concluida, DataFrame carregado.\")\n",
    "\n",
    "\n",
    "  print(\"\\nETAPA 02: Transformação e Validação dos dados\")\n",
    "  print(\"Executando...\")\n",
    "  df_silver = transform_and_validate(df_raw, schema)\n",
    "\n",
    "  if df_silver is None: \n",
    "    print(\"FALHA NO PIPELINE: Transformação e Validação.\")\n",
    "    return False \n",
    "  \n",
    "  print(\"\\nETAPA 03: Carregamento dos dados (LOAD)\")\n",
    "  success = load(df_silver, ddl_script_path, engine)\n",
    "  if not success:\n",
    "    print(\"FALHA NO PIPELINE: Carregamento (LOAD)\")\n",
    "\n",
    "  print(\"\\nPIPELINE CONCLUÍDA COM SUCESSO!\")\n",
    "  return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "899345fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ETAPA 01: Extração de dados\n",
      "Executando...\n",
      "Extração concluida, DataFrame carregado.\n",
      "\n",
      "ETAPA 02: Transformação e Validação dos dados\n",
      "Executando...\n",
      "ERRO! O DataFrame não foi validado: [PACKAGE_NOT_INSTALLED] PyArrow >= 11.0.0 must be installed; however, it was not found.\n",
      "FALHA NO PIPELINE: Transformação e Validação.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_pipeline(DATA_PATH, DDL_PATH, engine, schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
